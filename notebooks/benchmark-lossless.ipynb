{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476a16e6",
   "metadata": {},
   "source": [
    "# Benchmark lossless compression strategies\n",
    "\n",
    "In this notebook we analyze the performance of different lossless compression strategies in terms of:\n",
    "\n",
    "* compression ratio (n_bytes / n_stored_bytes)\n",
    "* compression speed\n",
    "* decompression speed\n",
    "\n",
    "The lossless compression algorithms compared are:\n",
    "\n",
    "- Zarr BLOSC compressors\n",
    "    * lz4 \n",
    "    * lz4hc \n",
    "    * zlib\n",
    "    * zstd\n",
    "- Audio compressor\n",
    "    * FLAC\n",
    "    * WavPack\n",
    "    \n",
    "The zarr compressors are implemented via the SpikeInterface `save(format=\"zarr\")` function and run with different options:\n",
    "\n",
    "* level (low - 1, medium - 5, high - 9)\n",
    "* BLOSC shuffle filter (no, auto/shuffle, bit)\n",
    "\n",
    "\n",
    "Custom numcodecs wrapper have also been written for FLAC and WavPack with the following levels/options:\n",
    "\n",
    "* flac (low - 1, medium - 5, high - 8)\n",
    "* wavpack (low - f, medium - h, high - hh)\n",
    "\n",
    "Since pyFLAC supports 2 channels at most, we test 2 options for FLAC:\n",
    "\n",
    "    - chunking by time only (and flatten the data)\n",
    "    - chunk size is (num_samples, 2) --> FLAC compresses streams of \"stereo\" channels\n",
    "\n",
    "All compressors are run with different chunk sizes (0.1s, 1s, 10s). Additionally, compression is run for two cases:\n",
    "\n",
    "- raw data (no preprocessing) - lsb = False\n",
    "- median subtraction and LSB division - lsb = True\n",
    "\n",
    "\n",
    "This notebook assumes the `ephys-compression/scripts/benchmark-lossless.py` has been run and the `ephys-compression/data/results/benchmark-lossless.csv\"` is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import prettify_axes\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c65c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = True\n",
    "\n",
    "fig_folder = Path(\".\") / \"figures\" / \"lossless\"\n",
    "fig_folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d54b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(\"../data/results/benchmark-lossless-final.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8423b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0823ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "blosc_compressors = np.unique(res.query(\"compressor_type == 'blosc'\").compressor)\n",
    "numcodecs_compressors = np.unique(res.query(\"compressor_type == 'numcodecs'\").compressor)\n",
    "audio_compressors = np.unique(res.query(\"compressor_type == 'audio'\").compressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafc320",
   "metadata": {},
   "source": [
    "# LSB correction?\n",
    "\n",
    "For Open Ephys saved data, the `int16` binary files actually have an lsb > 1:\n",
    "\n",
    "- NP1: lsb = 12 --> ~2.34 uV\n",
    "- NP2: lab = 3 --> ~0.585 uV\n",
    "\n",
    "While this does not affect the signals, it might affect compression because more bits than needed are used to encode for the voltage values.\n",
    "\n",
    "For NP1, in addition, the channel signals are not always centered at 0, meaning that one channel could have **central** values of (-12, 0, 12) and another channel could have, for instance (-11, 1, 13). For NP2, this is not the case, but many channels are not centered at 0.\n",
    "\n",
    "In order to account for this, we first estimate the median for each channel using chunks of the data and, before compression, we subtract the median and divide the signals by the LSB.\n",
    "\n",
    "At decompression, to recover the original data, the signals are rescaled by the LSB and the median is re-added. Note that these last two steps are not necessary:\n",
    "\n",
    "- the median values are irrelevant for downstrem analysis\n",
    "- the LSB scaling can be accounted for simply by resetting the `gain` values with the initial scaling\n",
    "\n",
    "For these reasons, the decompression speeds displayed below are an over-estimation of the actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8406577c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig_lsb_all, axs_lsb_all = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "for p, probe in enumerate(np.unique(res.probe)):\n",
    "    dset_probe = res.query(f\"probe == '{probe}'\")\n",
    "    \n",
    "    sns.boxplot(data=dset_probe, x=\"compressor_type\", y=\"CR\", hue=\"lsb\", ax=axs_lsb_all[p])\n",
    "\n",
    "    axs_lsb_all[p].set_title(probe, fontsize=20)\n",
    "fig_lsb_all.subplots_adjust(wspace=0.3)  \n",
    "prettify_axes(axs_lsb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73804827",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_lsb_all.savefig(fig_folder / \"lsb_corr.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lsb = res.query(\"lsb == True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_blosc = res_lsb.query(f\"compressor_type == 'blosc'\")\n",
    "res_numcodecs = res_lsb.query(f\"compressor_type == 'numcodecs'\")\n",
    "res_audio = res_lsb.query(f\"compressor_type == 'audio'\")\n",
    "\n",
    "rec_zarr = res_lsb.query(f\"compressor_type != 'audio'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74145802",
   "metadata": {},
   "source": [
    "# ZARR \n",
    "\n",
    "We start by comparing compression options readily available via the Blosc meta-compressor in ZARR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b78dc6",
   "metadata": {},
   "source": [
    "### What is the best ZARR-based option in terms of CR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be944d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = rec_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa616f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_np1 = dset.query(\"probe == 'Neuropixels1.0'\")\n",
    "res_np2 = dset.query(\"probe == 'Neuropixels2.0'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bdc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNP1\\n\")\n",
    "print(res_np1.iloc[np.argmax(res_np1.CR)])\n",
    "print(\"\\nNP2\\n\")\n",
    "print(res_np2.iloc[np.argmax(res_np2.CR)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7d63b",
   "metadata": {},
   "source": [
    "For both NP1 and NP2, the best zarr compressor is **Blosc-zstd - level 9 - chunk 1s/10s - shuffle BIT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ed0c9",
   "metadata": {},
   "source": [
    "### Effect of shuffling options\n",
    "\n",
    "The BLOSC meta-compressor provides two shuffling options:\n",
    "\n",
    "- byte shuffle (shuffle)\n",
    "- bit shuffle\n",
    "\n",
    "A byte shuffle is also available via the `numcodecs.Shuffle` for other non-blosc codecs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sh_all, axs_sh_all = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))\n",
    "sns.boxplot(data=dset, x=\"shuffle\", y=\"CR\", ax=axs_sh_all[0, 0])\n",
    "sns.boxplot(data=dset, x=\"shuffle\", y=\"xRT\", ax=axs_sh_all[0, 1])\n",
    "sns.boxplot(data=dset, x=\"shuffle\", y=\"D-10s\", ax=axs_sh_all[0, 2])\n",
    "\n",
    "sns.boxplot(data=dset, x=\"compressor\", y=\"CR\", hue=\"shuffle\", ax=axs_sh_all[1, 0])\n",
    "sns.boxplot(data=dset, x=\"compressor\", y=\"xRT\", hue=\"shuffle\", ax=axs_sh_all[1, 1])\n",
    "sns.boxplot(data=dset, x=\"compressor\", y=\"D-10s\", hue=\"shuffle\", ax=axs_sh_all[1, 2])\n",
    "\n",
    "axs_sh_all[1, 0].set_xticklabels(axs_sh_all[1, 0].get_xticklabels(), rotation=90)\n",
    "axs_sh_all[1, 1].set_xticklabels(axs_sh_all[1, 1].get_xticklabels(), rotation=90)\n",
    "axs_sh_all[1, 2].set_xticklabels(axs_sh_all[1, 2].get_xticklabels(), rotation=90)\n",
    "\n",
    "fig_sh_all.subplots_adjust(wspace=0.3)\n",
    "\n",
    "fig_sh_all.suptitle(\"Shuffling\", fontsize=20)\n",
    "\n",
    "prettify_axes(axs_sh_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf27e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_sh_all.savefig(fig_folder / \"shuffling.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc58f8",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "In general, pre-shuffling (byte or bit) the data improves conversion performance with respect to no pre-shuffling.\n",
    "The `BIT shuffle` available in blosc seem to be the best option, as it provides better CR, compression and decompression speed. \n",
    "\n",
    "Let's focus on that (and therefore on BLOSC compressors) for the rest of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_shuffle = \"bit\"\n",
    "dset_shuffle = dset.query(f\"shuffle == '{selected_shuffle}'\")\n",
    "dset_shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db016a11",
   "metadata": {},
   "source": [
    "### Effect of chunk duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_ch_all, axs_ch_all = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))\n",
    "sns.boxplot(data=dset_shuffle, x=\"chunk_dur\", y=\"CR\", ax=axs_ch_all[0, 0])\n",
    "sns.boxplot(data=dset_shuffle, x=\"chunk_dur\", y=\"xRT\", ax=axs_ch_all[0, 1])\n",
    "sns.boxplot(data=dset_shuffle, x=\"chunk_dur\", y=\"D-10s\", ax=axs_ch_all[0, 2])\n",
    "\n",
    "sns.boxplot(data=dset_shuffle, x=\"compressor\", y=\"CR\", hue=\"chunk_dur\", ax=axs_ch_all[1, 0])\n",
    "sns.boxplot(data=dset_shuffle, x=\"compressor\", y=\"xRT\", hue=\"chunk_dur\", ax=axs_ch_all[1, 1])\n",
    "sns.boxplot(data=dset_shuffle, x=\"compressor\", y=\"D-10s\", hue=\"chunk_dur\", ax=axs_ch_all[1, 2])\n",
    "\n",
    "axs_ch_all[1, 0].set_xticklabels(axs_ch_all[1, 0].get_xticklabels(), rotation=90)\n",
    "axs_ch_all[1, 1].set_xticklabels(axs_ch_all[1, 1].get_xticklabels(), rotation=90)\n",
    "axs_ch_all[1, 2].set_xticklabels(axs_ch_all[1, 2].get_xticklabels(), rotation=90)\n",
    "\n",
    "fig_ch_all.subplots_adjust(wspace=0.3)\n",
    "fig_ch_all.suptitle(\"Chunk duration\", fontsize=20)\n",
    "\n",
    "prettify_axes(axs_ch_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd58e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_ch_all.savefig(fig_folder / \"chunks.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cafbba1",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "Chunk duration seems to be relatively irrelevant for compression metrics. So let's pick 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_chunk = \"1s\"\n",
    "dset_chunk = dset_shuffle.query(f\"chunk_dur == '{selected_chunk}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e13e4",
   "metadata": {},
   "source": [
    "Let's now confirm that the level does its job..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428eb0ea",
   "metadata": {},
   "source": [
    "### Effect of compression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63622387",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_lev_all, axs_lev_all = plt.subplots(ncols=3, nrows=1, figsize=(15, 6))\n",
    "sns.boxplot(data=dset_chunk, x=\"level\", y=\"CR\", hue=\"compressor\", ax=axs_lev_all[0])\n",
    "sns.boxplot(data=dset_chunk, x=\"level\", y=\"xRT\", hue=\"compressor\", ax=axs_lev_all[1])\n",
    "sns.boxplot(data=dset_chunk, x=\"level\", y=\"D-10s\", hue=\"compressor\", ax=axs_lev_all[2])\n",
    "fig_lev_all.subplots_adjust(wspace=0.3)\n",
    "fig_lev_all.suptitle(\"Compression level\", fontsize=20)\n",
    "\n",
    "prettify_axes(axs_lev_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_lev_all.savefig(fig_folder / \"levels.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_zlib, ax_zlib = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "res_zlib = dset.query(\"compressor == 'blosc-zlib' and chunk_dur == '1s'\")\n",
    "\n",
    "sns.barplot(data=res_zlib, x=\"level\", y=\"CR\", hue=\"shuffle\", ax=ax_zlib)\n",
    "ax_zlib.set_title(\"BLOSC-ZLIB - compression level\", fontsize=20)\n",
    "prettify_axes(ax_zlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddfa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_zlib.savefig(fig_folder / \"zlib_level.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40a810",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "For most compressors, compression level does its job (increasing levels yield increasing CR). Strangely, for `blosc-zlib` the level seems to have the opposite effect. Of course, the higher the level the slower the compression speed. Decompression speed doesn't seem to be affected (this is not the case for the `numcodecs.Zlib` wrapper, that doesn't play well with BIT shuffling).\n",
    "\n",
    "For the final analysis, let's pick level 9 and compare the raw compression with the median+lsb preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631205c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_level = \"high\"\n",
    "dset_level = dset_chunk.query(f\"level == '{selected_level}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48306efc",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "For both NP1 and NP2, the LSB correction significantly improves CRs.\n",
    "\n",
    "- NP1: from 2.1 ($\\sim$47% size) to 3.13 ($\\sim$32% size) \n",
    "- NP2: from 1.5 ($\\sim$66% size) to 1.88 ($\\sim$53% size)\n",
    "\n",
    "Compression speed is reduced (especially for `lz4`) due to the preprocessing (which requires upcasting to float, scaling, and downcasting back to int16)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac911e2",
   "metadata": {},
   "source": [
    "As a final step, we select LSB and Zstd as best options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd08f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_compressor = \"blosc-zstd\"\n",
    "dset_best_zarr = dset_level.query(f\"compressor == '{selected_compressor}'\")\n",
    "dset_best_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset_best_zarr.to_latex(index=False, columns=[\"probe\", \"duration\", \"compressor_type\",\n",
    "                                                    \"compressor\", \"level\", \"shuffle\", \"chunk_dur\",\n",
    "                                                    \"CR\", \"xRT\", \"D-10s\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1696124",
   "metadata": {},
   "source": [
    "# AUDIO compression\n",
    "\n",
    "Lossless audio codecs could provide a good alternative to general-purpose compression algorithms because: \n",
    "\n",
    "- Audio signals are also timeseries \n",
    "- Frequiency range is similar\n",
    "- Multiple channels are correlated\n",
    "\n",
    "We tried to use FLAC and WavPack. FLAC supports up to 2 channels with pyFLAC, so we need to either:\n",
    "- flatten multi-channel signals into (channel_chunk_size=-1)\n",
    "- save blocks with 2 channels (channel_chunk_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6913b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_audio = res_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c791ec1",
   "metadata": {},
   "source": [
    "No shuffling is available in FLAC, so we just select the same chunk duration for the comparison. \n",
    "WavPack doesn't have a compression level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc351fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_chunk = \"1s\"\n",
    "dset_audio_chunk = dset_audio.query(f\"chunk_dur == '{selected_chunk}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66ee92",
   "metadata": {},
   "source": [
    "### FLAC: flattening or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_flac = dset_audio_chunk.query(\"compressor == 'flac'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_flac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_flac, axs_flac = plt.subplots(ncols=3, figsize=(15, 6))\n",
    "\n",
    "sns.boxplot(data=dset_flac, x=\"channel_chunk_size\", y=\"CR\", ax=axs_flac[0])\n",
    "sns.boxplot(data=dset_flac, x=\"channel_chunk_size\", y=\"xRT\", ax=axs_flac[1])\n",
    "sns.boxplot(data=dset_flac, x=\"channel_chunk_size\", y=\"D-10s\", ax=axs_flac[2])\n",
    "fig_flac.suptitle(\"Channel chunk size (flac)\", fontsize=20)\n",
    "\n",
    "prettify_axes(axs_flac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_flac.savefig(fig_folder / \"flac_chunks.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908912af",
   "metadata": {},
   "source": [
    "**COMMENT**\n",
    "\n",
    "Flattening or using \"stereo\" channels doesn't seem to make a difference for CR. Actually, compression and decompression speeds are slightly faster when stereo mode is enabled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_wavpack = dset_audio_chunk.query(\"compressor == 'wavpack'\")\n",
    "dset_flac_2 = dset_flac.query(\"channel_chunk_size == 2\") # wavpack has chunk_channel_size == -1\n",
    "dset_audio_2 = pd.concat([dset_flac_2, dset_wavpack])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d3600",
   "metadata": {},
   "source": [
    "### Effect of compression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b54adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_lev_all, axs_lev_all = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))\n",
    "\n",
    "sns.barplot(data=dset_audio_2.query(\"probe == 'Neuropixels1.0'\"), x=\"compressor\", \n",
    "            y=\"CR\", hue=\"level\", ax=axs_lev_all[0, 0])\n",
    "sns.barplot(data=dset_audio_2.query(\"probe == 'Neuropixels1.0'\"), x=\"compressor\", \n",
    "            y=\"xRT\", hue=\"level\", ax=axs_lev_all[0, 1])\n",
    "sns.barplot(data=dset_audio_2.query(\"probe == 'Neuropixels1.0'\"), x=\"compressor\", \n",
    "            y=\"D-10s\", hue=\"level\", ax=axs_lev_all[0, 2])\n",
    "\n",
    "sns.barplot(data=dset_audio_2.query(\"probe == 'Neuropixels2.0'\"), x=\"compressor\", \n",
    "            y=\"CR\", hue=\"level\", ax=axs_lev_all[1, 0])\n",
    "sns.barplot(data=dset_audio_2.query(\"probe == 'Neuropixels2.0'\"), x=\"compressor\", \n",
    "            y=\"xRT\", hue=\"level\", ax=axs_lev_all[1, 1])\n",
    "sns.barplot(data=dset_audio_2.query(\"probe == 'Neuropixels2.0'\"), x=\"compressor\", \n",
    "            y=\"D-10s\", hue=\"level\", ax=axs_lev_all[1, 2])\n",
    "\n",
    "axs_lev_all[0, 1].set_title(\"Neuropixels 1.0\", fontsize=18)\n",
    "axs_lev_all[1, 1].set_title(\"Neuropixels 2.0\", fontsize=18)\n",
    "\n",
    "\n",
    "fig_lev_all.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "fig_lev_all.suptitle(\"Compression level\", fontsize=20)\n",
    "\n",
    "prettify_axes(axs_lev_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_audio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab107b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_lev_all.savefig(fig_folder / \"audio_levels.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42df18",
   "metadata": {},
   "source": [
    "There is a slight increase in compression performance with compression level from low to medium, but not so between medium an high.  For FLAC, compression speed does not seem to by affected compression level.\n",
    "However, for WavPack, decompression speed is a bit slow (~0.5 xRT), probably due to the sub-optimal implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6957c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_level = \"medium\"\n",
    "dset_best_audio = dset_audio_2.query(f\"level == '{selected_level}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1596601",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_best_all = pd.concat([dset_best_zarr, dset_best_audio])\n",
    "dset_best_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68670e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_best_all, axs_best_all = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))\n",
    "\n",
    "sns.barplot(data=dset_best_all.query(\"probe == 'Neuropixels1.0'\"), x=\"compressor\", \n",
    "            y=\"CR\", ax=axs_best_all[0, 0])\n",
    "sns.barplot(data=dset_best_all.query(\"probe == 'Neuropixels1.0'\"), x=\"compressor\", \n",
    "            y=\"xRT\", ax=axs_best_all[0, 1])\n",
    "sns.barplot(data=dset_best_all.query(\"probe == 'Neuropixels1.0'\"), x=\"compressor\", \n",
    "            y=\"D-10s\", ax=axs_best_all[0, 2])\n",
    "\n",
    "sns.barplot(data=dset_best_all.query(\"probe == 'Neuropixels2.0'\"), x=\"compressor\", \n",
    "            y=\"CR\", ax=axs_best_all[1, 0])\n",
    "sns.barplot(data=dset_best_all.query(\"probe == 'Neuropixels2.0'\"), x=\"compressor\", \n",
    "            y=\"xRT\", ax=axs_best_all[1, 1])\n",
    "sns.barplot(data=dset_best_all.query(\"probe == 'Neuropixels2.0'\"), x=\"compressor\", \n",
    "            y=\"D-10s\",  ax=axs_best_all[1, 2])\n",
    "\n",
    "axs_best_all[0, 1].set_title(\"Neuropixels 1.0\", fontsize=18)\n",
    "axs_best_all[1, 1].set_title(\"Neuropixels 2.0\", fontsize=18)\n",
    "\n",
    "prettify_axes(axs_best_all)\n",
    "\n",
    "fig_best_all.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "fig_best_all.suptitle(\"Compression strategies\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_best_all.savefig(fig_folder / \"best.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d19b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for probe in np.unique(dset_level.probe):\n",
    "    dset_probe = dset_best_all.query(f\"probe == '{probe}'\")\n",
    "    \n",
    "    print(f\"\\n\\n{probe}\\n\")\n",
    "    print(dset_probe.groupby(\"compressor\")[\"CR\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_best_sorted = dset_best_all.sort_values(\"probe\")\n",
    "\n",
    "dset_best_sorted[\"file_size\"] = (1 / dset_best_sorted[\"CR\"]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c824447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset_best_sorted.to_latex(index=False, columns=[\"probe\", \"duration\", \n",
    "                                                    \"compressor\", \"level\", \"shuffle\", \"chunk_dur\",\n",
    "                                                    \"CR\", \"file_size\", \"xRT\", \"D-10s\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8d0a2",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846be4bb",
   "metadata": {},
   "source": [
    "In terms of CR, FLAC reaches the highest lossless compression for NP1 (3.462 -- $\\sim$29% size), but it is extremely slow to decompress. Note that Zstd, with its 3.13 CR, reduced the size to $\\sim$32%. WavPack is somewhere in the middle (CR 3.36 -- $\\sim$30% size). \n",
    "\n",
    "For NP2, WavPack can reduce to a $\\sim$42% size in contrast to a $\\sim$53% of Zstd. Decompression is currently a bit slow also for WavPack (around 5s to retrieve 10s of traces), but this can be probably improved by bypassing the temporary wav conversion and binding the WavPack C library directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
